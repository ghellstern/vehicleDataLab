{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%AddDeps org.apache.spark spark-streaming-kafka-0-10_2.11 2.0.2\n",
    "\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.util.LongAccumulator\n",
    "import org.apache.spark.streaming.kafka010._\n",
    "import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\n",
    "import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n",
    "\n",
    "// Parameters to connect to Message Hub using the Kafka client\n",
    "// Note the sasl.jaas.config attribute which is new in Kafka 0.10.2\n",
    "// >>>>> REPLACE $MH_USER_ID$ and $MH_PASSWORD$ with your Message Hub credentials\n",
    "val kafkaParams = Map[String, Object](\n",
    "  \"bootstrap.servers\" -> \"kafka01-prod01.messagehub.services.us-south.bluemix.net:9093\",\n",
    "  \"key.deserializer\" -> classOf[StringDeserializer],\n",
    "  \"value.deserializer\" -> classOf[StringDeserializer],\n",
    "  \"group.id\" -> \"streaminganalysis2\",\n",
    "  \"security.protocol\" -> \"SASL_SSL\",\n",
    "  \"sasl.mechanism\" -> \"PLAIN\",\n",
    "  \"ssl.protocol\" -> \"TLSv1.2\",\n",
    "  \"ssl.enabled.protocols\" -> \"TLSv1.2\",\n",
    "  \"ssl.endpoint.identification.algorithm\" -> \"HTTPS\",\n",
    "  \"sasl.jaas.config\" -> \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"$MH_USER_ID$\\\" password=\\\"$MH_PASSWORD$\\\";\",\n",
    "  \"auto.offset.reset\" -> \"latest\",\n",
    "  \"enable.auto.commit\" -> (false: java.lang.Boolean)\n",
    ")\n",
    "\n",
    "// The topic name for the events\n",
    "val topics = Array(\"carevents\")\n",
    "\n",
    "\n",
    "// Function to create StreamingContext properly\n",
    "def creatingFunc(sc: SparkContext): StreamingContext = {\n",
    "  // Batch interval is 5 seconds\n",
    "  val ssc = new StreamingContext(sc, Seconds(5))\n",
    "\n",
    "  // Subscribe to the topic and generate a stream of messages every batch interval\n",
    "  val stream = KafkaUtils.createDirectStream[String, String](\n",
    "    ssc,\n",
    "    PreferConsistent,\n",
    "    Subscribe[String, String](topics, kafkaParams)\n",
    "  )\n",
    "\n",
    "  // The stream consists of Kafka ConsumerRecord elements. Just want the value from the messages.\n",
    "  val events = stream.map(record => (record.value))\n",
    "\n",
    "  // Events look like this:\n",
    "  //  {  \"vehicleNumber\" : \"car8\",  \"fuelRemaining\" : 4.3912726149684245,  \"fuelCapacity\" : 11.0,  \"distance\" : 77060.38816165098,  \"refills\" : 0,  \"ts\" : \"2017-04-03T08:51:38.903\"}\n",
    "\n",
    "  // Strip out the JSON formatting characters to give plain strings to look like this:\n",
    "  //  vehicleNumber:car8,fuelRemaining:4.3912726149684245,fuelCapacity:11.0,distance:77060.38816165098,refills:0,ts:2017-04-03T08:51:38.903\n",
    "  val eventsAsStrings = events.map(_.replaceAll(\"[{ }\\\"\\u00A0]\", \"\"))\n",
    "\n",
    "  // Split to give an array of strings per value to look like this:\n",
    "  //  Array(vehicleNumber:car8, fuelRemaining:4.3912726149684245, fuelCapacity:11.0, distance:77060.38816165098, refills:0, ts:2017-04-03T08:51:38.903)\n",
    "  val eventsAsPerValueStrings = eventsAsStrings.map(_.split(','))\n",
    "\n",
    "  // Split each string into another array of strings to look like this:\n",
    "  //  Array(Array(vehicleNumber, car8),\n",
    "  //        Array(fuelRemaining, 4.3912726149684245),\n",
    "  //        Array(fuelCapacity, 11.0),\n",
    "  //        Array(distance, 77060.38816165098),\n",
    "  //        Array(refills, 0),\n",
    "  //        Array(ts, 2017-04-03T08, 51, 38.903))\n",
    "  val eventsAsSeparateStrings = eventsAsPerValueStrings.map(_.map(_.split(':')))\n",
    "\n",
    "  // And finally take each array of strings with exactly two members, convert into a Tuple, and then a Map to look like this:\n",
    "  //  Map(vehicleNumber -> car8,\n",
    "  //      fuelRemaining -> 4.3912726149684245,\n",
    "  //      fuelCapacity -> 11.0,\n",
    "  //      distance -> 77060.38816165098,\n",
    "  //      refills -> 0)\n",
    "  val eventsAsMaps = eventsAsSeparateStrings.map(_.collect({case Array(s0,s1) => (s0,s1)}).toMap)\n",
    "  eventsAsMaps.print\n",
    "\n",
    "  ssc\n",
    "}\n",
    "\n",
    "// Main initialisation, picking up context if it already exists\n",
    "val sc = SparkContext.getOrCreate()\n",
    "val ssc = StreamingContext.getActiveOrCreate(() => creatingFunc(sc))\n",
    "\n",
    "// Start the processing\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// This cell can be used to stop existing StreamingContexts. May be helpful when stopping and restarting the analytics.\n",
    "StreamingContext.getActive.foreach{_.stop(stopSparkContext = false)}\n",
    "println(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 with Spark 2.0",
   "language": "scala",
   "name": "scala-spark20"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}